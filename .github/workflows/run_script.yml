name: Run Web Scraping Script Every Hour

on:
  schedule:
    # 이 크론 표현식은 워크플로우가 매 시간마다 실행되도록 설정합니다.
    - cron: "0 * * * *"
  workflow_dispatch: # 수동 실행을 위한 설정

jobs:
  scrape_jobs:
    runs-on: ubuntu-latest

    steps:
      # Step 1: 레포지토리 코드 체크아웃
      - name: Checkout repository
        uses: actions/checkout@v2

      # Step 2: Python 3.9 설정
      - name: Set up Python 3.9
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'

      # Step 3: 필요한 패키지 설치
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install webdriver-manager  # 추가

      # Step 4: Chrome WebDriver 다운로드 (Chrome 133에 맞는 버전)
      - name: Download Chrome WebDriver
        run: |
          # 현재 설치된 Chrome 버전 확인
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}')
        
          # Chrome 버전에 맞는 ChromeDriver 버전 다운로드
          CHROME_DRIVER_VERSION=$(curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION)
        
          # ChromeDriver 다운로드 및 설치
          wget https://chromedriver.storage.googleapis.com/${CHROME_DRIVER_VERSION}/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/chromedriver
          sudo chmod +x /usr/local/bin/chromedriver

      # Step 5: 스크립트 실행
      - name: Run web scraping script
        run: |
          python script.py
